---
title: 'Gaussian Mixture Models (GMM) & Expectation–Maximization (EM)'
date: 2025-10-01
permalink: /posts/2025/10/blog-post-1/
tags:
  - machine learning
---

This note combines the CS 7641 course slides with complementary materials, reinforcing the understanding of **GMM** and the **EM algorithms**.

It will cover five main sections:

1. **Multivariate Gaussian basics** — the mathematical form and important properties.
2. **Gaussian Mixture Model (GMM)** — how it generalizes clustering with soft assignments.
3. **Expectation–Maximization (EM) for GMM** — the standard training algorithm.
4. **General EM as ELBO maximization** — the deeper theoretical view.
5. **Practical notes** — covariance choices, initialization, and diagnostics.

---

## Part A. Multivariate Gaussian Basics

Before understanding mixtures of Gaussians, let’s recall the form and key properties of a single multivariate Gaussian.

### A1. Probability Density Function

For a random vector $$x \in \mathbb{R}^d$$ with mean $$\mu \in \mathbb{R}^d$$ and covariance matrix $$\Sigma \in \mathbb{R}^{d \times d}$$ (symmetric, positive definite), the density function is:

$$
\mathcal{N}(x \mid \mu, \Sigma)
= \frac{1}{(2\pi)^{d/2}\lvert\Sigma\rvert^{1/2}}
\exp\!\left(-\frac{1}{2}(x-\mu)^{\top}\Sigma^{-1}(x-\mu)\right).
$$

Taking the logarithm (useful afterwards):

$$
\log \mathcal{N}(x \mid \mu, \Sigma)
= -\frac{1}{2} \Big[ d\log(2\pi) + \log\lvert\Sigma\rvert + (x-\mu)^{\top}\Sigma^{-1}(x-\mu)\Big].
$$

### A2. Covariance Structures

- **Spherical**: $$\Sigma = \sigma^2 I$$. Contours are circles (2D) or spheres (3D).  
- **Diagonal**: $$\Sigma = \mathrm{diag}(\sigma_1^2, \dots, \sigma_d^2)$$. Contours are axis-aligned ellipses.  
- **Full**: general covariance, rotated ellipses capturing correlations.  

### A3. Useful Properties

1. **Affine transformations** If $$y = Ax+b$$ and $$x \sim \mathcal{N}(\mu, \Sigma)$$, then $$y \sim \mathcal{N}(A\mu+b, A\Sigma A^{\top})$$.

2. **Marginal and conditional** A joint Gaussian remains Gaussian under marginalization and conditioning.

3. **MLE for mean and covariance** Given $$\{x_n\}_{n=1}^N$$ i.i.d. from a Gaussian,
   $$
   \hat{\mu} = \frac{1}{N}\sum_{n=1}^N x_n, 
   \quad
   \hat{\Sigma} = \frac{1}{N}\sum_{n=1}^N (x_n - \hat{\mu})(x_n - \hat{\mu})^{\top}.
   $$

---

## Part B. Gaussian Mixture Model (GMM)

Now that we understand single Gaussians, we can build mixtures to model more complex data distributions.

### B1. Hard vs. Soft Assignments

K-means clustering assigns each data point to the nearest centroid (hard assignment), but it has clear limitations.  
To address these, we move to a **soft assignment** perspective.

Define the **responsibility**:
$$
\gamma_{nk} = \Pr(z_n = k \mid x_n, \theta),
$$
which quantifies how strongly the $$k$$-th cluster “claims” the point $$x_n$$.

Arrange all responsibilities into a matrix:
$$
\Gamma =
\begin{pmatrix}
\gamma_{11} & \gamma_{12} & \cdots & \gamma_{1K} \\
\gamma_{21} & \gamma_{22} & \cdots & \gamma_{2K} \\
\vdots & \vdots & \ddots & \vdots \\
\gamma_{N1} & \gamma_{N2} & \cdots & \gamma_{NK}
\end{pmatrix}_{N \times K}.
$$

Each row sums to 1:
$$
\sum_{k=1}^K \gamma_{nk} = 1.
$$

### B2. The Mathematical Form of GMM

The mixture distribution is:
$$
p(x)=\sum_{k=1}^K \pi_k\,\mathcal{N}(x\mid\mu_k,\Sigma_k),
$$
with $$\pi_k \ge 0$$ and $$\sum_{k=1}^K \pi_k = 1$$.

Generative process:
$$
z \sim \mathrm{Categorical}(\pi_1,\ldots,\pi_K), 
\qquad
x \mid (z=k) \sim \mathcal{N}(\mu_k,\Sigma_k).
$$

Posterior responsibility:
$$
\gamma_{nk} =
\frac{\pi_k\,\mathcal{N}(x_n\mid\mu_k,\Sigma_k)}
{\sum_{j=1}^K \pi_j\,\mathcal{N}(x_n\mid\mu_j,\Sigma_j)}.
$$

Complete-data log-likelihood (if $$z$$ observed):
$$
\log p_\theta(\{x_n,z_n\}_{n=1}^N)
=\sum_{n=1}^N \sum_{k=1}^K \mathbf{1}_{\{z_n=k\}}
\Big[\log \pi_k+\log \mathcal{N}(x_n\mid\mu_k,\Sigma_k)\Big].
$$

Incomplete (marginal) log-likelihood:
$$
\mathcal{L}(\theta)
=\sum_{n=1}^N \log\!\Big(\sum_{k=1}^K \pi_k\,\mathcal{N}(x_n\mid\mu_k,\Sigma_k)\Big).
$$

---

## Part C. EM Algorithm for GMM

Let $$N_k = \sum_{n=1}^N \gamma_{nk}$$.

### Step 1. E-step
$$
\gamma_{nk} =
\frac{\pi_k \, \mathcal{N}(x_n \mid \mu_k, \Sigma_k)}
{\sum_{j=1}^K \pi_j \, \mathcal{N}(x_n \mid \mu_j, \Sigma_j)}.
$$

### Step 2. M-step
$$
\pi_k = \frac{N_k}{N}, 
\qquad
\mu_k = \frac{1}{N_k} \sum_{n=1}^N \gamma_{nk} x_n, 
\qquad
\Sigma_k = \frac{1}{N_k} \sum_{n=1}^N \gamma_{nk}(x_n-\mu_k)(x_n-\mu_k)^{\top}.
$$

### Step 3. Convergence
EM guarantees monotone increase of the log-likelihood until convergence (local optimum).

---

## Part D. General EM via ELBO

We want to maximize:
$$
\log p_\theta(x) = \log \sum_{z} p_\theta(x, z).
$$

By Jensen’s inequality:
$$
\log p_\theta(x) 
\geq \sum_{z} q(z) \log \frac{p_\theta(x,z)}{q(z)} = \mathcal{F}(q, \theta).
$$

ELBO:
$$
\mathcal{F}(q, \theta) = \mathbb{E}_q[\log p_\theta(x, z)] + H(q).
$$

**EM as coordinate ascent**

- **E-step:** $$
  q(z) = p_\theta(z \mid x).
  $$

- **M-step:** $$
  \text{maximize } \mathbb{E}_q[\log p_\theta(x, z)].
  $$

Monotonicity:
$$
\log p_{\theta^{\text{new}}}(x)
\geq \mathcal{F}(q, \theta^{\text{new}})
\geq \mathcal{F}(q, \theta^{\text{old}})
= \log p_{\theta^{\text{old}}}(x).
$$

---

## Summary

- **GMM density:**

  $$
  p(x) = \sum_{k=1}^K \pi_k \,\mathcal{N}(x \mid \mu_k, \Sigma_k).
  $$

- **E-step:**

  $$
  \gamma_{nk} = \frac{\pi_k \,\mathcal{N}(x_n \mid \mu_k, \Sigma_k)}
  {\sum_{j=1}^K \pi_j \,\mathcal{N}(x_n \mid \mu_j, \Sigma_j)}.
  $$

- **M-step updates:**

  $$
  \pi_k = \frac{N_k}{N}, \qquad
  \mu_k = \frac{1}{N_k} \sum_{n=1}^N \gamma_{nk} x_n, \qquad
  \Sigma_k = \frac{1}{N_k} \sum_{n=1}^N \gamma_{nk}(x_n-\mu_k)(x_n-\mu_k)^{\top}.
  $$

- **General EM:** EM = coordinate ascent on ELBO; guarantees monotone likelihood increase.