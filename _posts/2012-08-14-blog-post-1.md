---
title: 'Gaussian Mixture Models (GMM) & Expectation–Maximization (EM)'
date: 2025-10-01
permalink: /posts/2025/10/blog-post-1/
tags:
  - machine learning
---

This note combines the CS 7641 course slides with additional theory, and will cover five main sections:

1. **Multivariate Gaussian basics** — the mathematical form and important properties.
2. **Gaussian Mixture Model (GMM)** — how it generalizes clustering with soft assignments.
3. **Expectation–Maximization (EM) for GMM** — the standard training algorithm.
4. **General EM as ELBO maximization** — the deeper theoretical view.
5. **Practical notes** — covariance choices, initialization, and diagnostics.



## Part A. Multivariate Gaussian Basics

Before understanding mixtures of Gaussians, let’s recall the form and key properties of a single multivariate Gaussian.

### A1. Probability Density Function

For a random vector $x \in \mathbb{R}^d$ with mean $\mu \in \mathbb{R}^d$ and covariance matrix $\Sigma \in \mathbb{R}^{d \times d}$ (symmetric, positive definite), the density function is:

$$
\mathcal{N}(x \mid \mu, \Sigma)
= \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}
\exp\!\left(-\tfrac12 (x-\mu)^\top\Sigma^{-1}(x-\mu)\right).
$$

Taking the logarithm (useful afterwards):

$$
\log \mathcal{N}(x \mid \mu, \Sigma)
= -\tfrac12 \Big[ d\log(2\pi) + \log|\Sigma| + (x-\mu)^\top\Sigma^{-1}(x-\mu)\Big].
$$

### A2. Covariance Structures

- **Spherical**: $\Sigma = \sigma^2 I$. Contours are circles (2D) or spheres (3D).
- **Diagonal**: $\Sigma = \text{diag}(\sigma_1^2, \dots, \sigma_d^2)$. Contours are axis-aligned ellipses.
- **Full**: general covariance, rotated ellipses capturing correlations.

<!-- TODO: Add figure illustrating spherical vs diagonal vs full covariance -->


### A3. Useful Properties

1. **Affine transformations**:  
   If $y = Ax+b$ and $x \sim \mathcal{N}(\mu, \Sigma)$, then  
   $$y \sim \mathcal{N}(A\mu+b, A\Sigma A^\top).$$

2. **Marginal and conditional**:  
   A joint Gaussian remains Gaussian under marginalization and conditioning.  
   This makes Gaussians analytically convenient.

3. **MLE for mean and covariance**:  
   Given $\{x_n\}_{n=1}^N$ i.i.d. from a Gaussian,  
   $$
   \hat{\mu} = \frac{1}{N}\sum_n x_n, 
   \quad
   \hat{\Sigma} = \frac{1}{N}\sum_n (x_n - \hat{\mu})(x_n - \hat{\mu})^\top.
   $$

## Part B. Gaussian Mixture Model (GMM)

Now that we understand single Gaussians, we can build mixtures to model more complex data distributions. The key motivation is that real-world data often follows **multi-modal distribution** and cannot be captured by a single Gaussian with one mean and covariance. By mixing several Gaussians together, we can approximate arbitrarily complicated shapes.

### B1. Hard v.s. Soft Assignments

K-means clustering assigns each data point to the nearest centroid (hard assignment), but it has clear limitations:

- It assumes spherical clusters of equal size.
- It cannot model overlapping clusters.
- It forces a point to belong exclusively to one cluster even if it's ambiguous.

To address these, we move to a **soft assignment** perspective. Instead of a binary assignment, each point can partially belong to multiple clusters. This flexibility allows modeling ambiguity and overlap in the data.

To quantify how likely each point is to belong to different clusters, we introduce **responsibilities**, which are essentially the posterior probabilities of cluster membership. These soft assignments bridge the gap between observed data and the hidden causes—we don’t force each point into one cluster, but allow it to be “explained” proportionally by all components, based on how well they fit. Ambiguous points near overlaps receive fractional weights, while well-separated points are close to one-hot.

We define

$$
\gamma_{nk} = \Pr(z_n = k \mid x_n, \theta),
$$

which quantifies how strongly the $k$-th cluster “claims” the point $x_n$. To present this more globally, we arrange all responsibilities into an $N \times K$ matrix:

$$
\Gamma =
\begin{pmatrix}
\gamma_{11} & \gamma_{12} & \cdots & \gamma_{1K} \\
\gamma_{21} & \gamma_{22} & \cdots & \gamma_{2K} \\
\vdots & \vdots & \ddots & \vdots \\
\gamma_{N1} & \gamma_{N2} & \cdots & \gamma_{NK}
\end{pmatrix}_{N \times K}.
$$

Here, the $n$-th row $(\gamma_{n1}, \dots, \gamma_{nK})$ gives the posterior probabilities over all clusters for $x_n$, and by construction each row sums to 1:

$$
\sum_{k=1}^K \gamma_{nk} = 1.
$$

This matrix form is useful: it highlights the soft membership structure across all points and clusters simultaneously.  


### B2. The Mathematical Form of GMM

We model the data density as a mixture of Gaussians so the distribution can express multiple modes:
$$
p(x)=\sum_{k=1}^K \pi_k\,\mathcal{N}(x\mid\mu_k,\Sigma_k),
$$
where $\pi_k\ge 0$ and $\sum_{k=1}^K\pi_k=1$, making $p(x)$ a valid pdf. Each component contributes probability mass in its own region, and the mixing weights $\{\pi_k\}$ control the prominence of the components.

To make the soft-assignment structure explicit, we introduce a latent categorical variable $z\in\{1,\dots,K\}$ that indexes components. In the generative view, each data point is produced in two steps:
$$
z \sim \mathrm{Categorical}(\pi_1,\ldots,\pi_K),
$$
$$
x \mid (z=k) \sim \mathcal{N}(\mu_k,\Sigma_k).
$$
This can immediately recover the mixture density above by marginalizing $z$.

Given an observed point $x_n$, the responsibility of component $k$ is the posterior membership probability. By Bayes’ rule,
$$
\gamma_{nk} \equiv p(z_n=k \mid x_n,\theta)
= \frac{\pi_k\,\mathcal{N}(x_n\mid\mu_k,\Sigma_k)}
{\sum_{j=1}^K \pi_j\,\mathcal{N}(x_n\mid\mu_j,\Sigma_j)}.
$$

The latent variable also clarifies **complete** vs **incomplete** data (only with both x and z known we call it complete). 

If cluster labels were observed, the complete-data log-likelihood would decompose additively:
$$
\log p_\theta(\{x_n,z_n\}_{n=1}^N)
=\sum_{n=1}^N \sum_{k=1}^K \mathbb{1}\{z_n=k\}\Big[\log \pi_k+\log \mathcal{N}(x_n\mid\mu_k,\Sigma_k)\Big].
$$
However, in practice $z_n$ is hidden, so we work with the incomplete (marginal) log-likelihood:
$$
\mathcal{L}(\theta)
=\sum_{n=1}^N \log\!\Big(\sum_{k=1}^K \pi_k\,\mathcal{N}(x_n\mid\mu_k,\Sigma_k)\Big),
$$
whose “log of a sum over components” prevents closed-form MLE (Maximum Likelihood Estimation) updates and motivates the EM algorithm that follows.


## Part C. EM Algorithm for GMM

Because the initial likelihood function is difficult to maximize directly, we turn to the EM algorithm. The key idea of EM is to exploit the latent variable structure: if we knew the hidden cluster assignments, parameter estimation would be straightforward. Since we don’t, EM alternates between estimating responsibilities (soft assignments) and updating parameters.

Let $N_k = \sum_{n=1}^N \gamma_{nk}$, which can be interpreted as the “effective number of points” assigned to cluster $k$.

### Step 1. E-step

We compute responsibilities under the current parameter estimates; at initialization, these are typically obtained from a k-means warm start:

$$
\gamma_{nk} \leftarrow
\frac{\pi_k \, \mathcal{N}(x_n \mid \mu_k, \Sigma_k)}
{\sum_{j=1}^K \pi_j \, \mathcal{N}(x_n \mid \mu_j, \Sigma_j)}.
$$

Intuitively, this step updates our belief about which component generated each data point, balancing both the prior $\pi_k$ and the fit of the Gaussian density.

### Step 2. M-step

Given these responsibilities, we update the parameters. Each update is essentially a weighted version of the MLE for Gaussians:

$$
\pi_k = \frac{N_k}{N}, 
\qquad \mu_k = \frac{1}{N_k} \sum_n \gamma_{nk} x_n, \qquad
\Sigma_k = \frac{1}{N_k} \sum_n \gamma_{nk}(x_n-\mu_k)(x_n-\mu_k)^\top.
$$

This means: update the mixing proportions $\pi_k$ by relative cluster sizes, update the means $\mu_k$ by responsibility-weighted averages, and update the covariances $\Sigma_k$ by responsibility-weighted scatter matrices.

### Step 3. Convergence

The EM algorithm guarantees that the observed-data (incomplete-data) log-likelihood is **non-decreasing** at each iteration. In practice, we monitor this original log-likelihood and declare convergence when its improvement falls below a small tolerance (or a maximum number of iterations is reached). EM converges to a stationary point, which is typically only a **local optimum**; consequently, results depend on initialization. Strong seeds—e.g., k-means centroids for means, with covariances and weights estimated from the induced partitions—are often essential, and multiple restarts can further improve robustness.




## Part D. General EM via ELBO

The EM algorithm for GMM is an instance of a more general principle. To appreciate this, let us take a step back and think about the general problem.

### D1. Log-sum Difficulty

We want to maximize the incomplete-data log-likelihood

$$
\log p_\theta(x) = \log \sum_z p_\theta(x, z).
$$

The summation over latent variables inside the logarithm makes direct optimization hard. EM resolves this by introducing a lower bound.

### D2. Variational Lower Bound (ELBO)

We introduce an auxiliary distribution $q(z)$ and apply Jensen’s inequality:

$$
\log p_\theta(x) 
= \log \sum_z q(z) \frac{p_\theta(x,z)}{q(z)} 
\geq \sum_z q(z) \log \frac{p_\theta(x,z)}{q(z)}=\mathcal{F}(q, \theta).
$$

The right-hand side is called the **Evidence Lower Bound (ELBO)**, which is concave:

$$
\mathcal{F}(q, \theta) = \mathbb{E}_q[\log p_\theta(x, z) - \log q(z)]= \mathbb{E}_q[\log p_\theta(x, z)] + H(q)=Q_q(\theta) + H(q),
$$

where $H(q)$ is the entropy of $q$. Intuitively, the ELBO is a tractable surrogate for the log-likelihood, and maximizing it pushes $q$ closer to the true posterior.

### D3. EM as Coordinate Ascent

EM can now be reinterpreted as coordinate ascent on the ELBO:

- **E-step**: with $\theta$ fixed, the best $q$ is the posterior $p_\theta(z \mid x)$. This makes the bound tight.
- **M-step**: with $q$ fixed, maximize the expected complete-data log-likelihood.

So EM alternates between tightening the bound and maximizing it, guaranteeing progress.


Because of this variational view, we can show that:

$$
\log p_{\theta^{\text{new}}}(x)
\geq \mathcal{F}(q, \theta^{\text{new}})
\geq \mathcal{F}(q, \theta^{\text{old}})
= \log p_{\theta^{\text{old}}}(x).
$$

Therefore, each EM iteration improves (or at least preserves) the likelihood. This explains the monotone convergence property of EM.

### D4. Discussion: Why EM for GMM Is an Optimization Procedure?

For GMM, EM becomes concrete: we just use the responsibilities and closed-form updates from part C.

**E-step (tighten the bound via posterior responsibilities).**  
Given current parameters $\theta^{(t)}=\{\pi_k^{(t)},\mu_k^{(t)},\Sigma_k^{(t)}\}_{k=1}^K$, compute responsibility:
$$
\gamma_{nk}^{(t)} \equiv p\!\left(z_n=k \mid x_n,\theta^{(t)}\right)
= \frac{\pi_k^{(t)}\,\mathcal{N}\!\left(x_n\mid \mu_k^{(t)},\Sigma_k^{(t)}\right)}
{\sum_{j=1}^K \pi_j^{(t)}\,\mathcal{N}\!\left(x_n\mid \mu_j^{(t)},\Sigma_j^{(t)}\right)},
\qquad \sum_{k=1}^K \gamma_{nk}^{(t)}=1 .
$$

**Define the GMM-specific $Q$-function (expected complete-data log-likelihood).**  
With these responsibilities,
$$
Q\!\left(\theta;\theta^{(t)}\right)
=\sum_{n=1}^N \sum_{k=1}^K \gamma_{nk}^{(t)}
\Big[\,\log \pi_k \;+\; \log \mathcal{N}(x_n\mid \mu_k,\Sigma_k)\Big].
$$

**M-step (improve the bound via maximizing $Q$).**  
Maximizing $Q(\theta;\theta^{(t)})$ over $\theta$ yields the **closed-form MLE updates**:
$$
N_k^{(t)}=\sum_{n=1}^N \gamma_{nk}^{(t)}, \qquad
\pi_k^{(t+1)}=\frac{N_k^{(t)}}{N},
$$
$$
\mu_k^{(t+1)}=\frac{1}{N_k^{(t)}}\sum_{n=1}^N \gamma_{nk}^{(t)}\,x_n,
\qquad
\Sigma_k^{(t+1)}=\frac{1}{N_k^{(t)}}\sum_{n=1}^N \gamma_{nk}^{(t)}
\big(x_n-\mu_k^{(t+1)}\big)\big(x_n-\mu_k^{(t+1)}\big)^\top .
$$

**Monotonicity (optimization viewpoint).**  
Instantiating the general EM argument with the GMM $Q$ above implies that the observed-data log-likelihood
$$
\mathcal{L}(\theta)\;=\;\sum_{n=1}^N \log\!\Big(\sum_{k=1}^K \pi_k\,\mathcal{N}(x_n\mid \mu_k,\Sigma_k)\Big)
$$
is **non-decreasing** across iterations:
each E-step uses the posterior responsibilities $\gamma_{nk}^{(t)}$ (tight bound), and each M-step maximizes $Q(\theta;\theta^{(t)})$ (bound improvement). Consequently, EM for GMM converges to a stationary point of $\mathcal{L}(\theta)$, with the attained optimum depending on initialization.


## Summary

- **GMM density**:  
  $$p(x) = \sum_k \pi_k \mathcal{N}(x \mid \mu_k, \Sigma_k).$$

- **E-step, calculate responsibilities**:  
  $$\gamma_{nk} = \frac{\pi_k \mathcal{N}(x_n \mid \mu_k, \Sigma_k)}{\sum_j \pi_j \mathcal{N}(x_n \mid \mu_j, \Sigma_j)}.$$

- **M-step updates**:  
  $$\pi_k = N_k / N, \quad
  \mu_k = \frac{1}{N_k} \sum_n \gamma_{nk} x_n, \quad
  \Sigma_k = \frac{1}{N_k} \sum_n \gamma_{nk}(x_n-\mu_k)(x_n-\mu_k)^\top.$$

- **General EM**:  
  EM = coordinate ascent on the ELBO.  
  Guarantees monotone likelihood increase.

